{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from Utils.Clustering.Cluster_Containments_Utils import *\n",
    "from Utils.Reference_Guided_Scaffolding.Scaffold_Using_References_Utils import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.patches import Rectangle\n",
    "rcParams = {'font.size': 20, \n",
    "            'font.weight': 'normal', \n",
    "            'font.family': 'sans-serif',\n",
    "            'axes.unicode_minus':False, \n",
    "            'axes.labelweight':'normal', \n",
    "            'xtick.labelsize' : 16,\n",
    "            'ytick.labelsize' : 16,\n",
    "            'axes.labelsize': 20,\n",
    "            'axes.spines.right' : False,\n",
    "            'axes.spines.top' : False,\n",
    "            'axes.spines.left' : False, \n",
    "           }\n",
    "\n",
    "plt.rcParams.update(rcParams)\n",
    "\n",
    "osa_len = 2932766\n",
    "osb_len = 3046682\n",
    "\n",
    "colors = {'C' : 'red', 'L':'blue', 'P':'olive', '-':'grey', 'M':'orange', 'I':'gold', \n",
    "          'S':'purple', 'H':'tan', 'J':'yellow', 'E':'cyan', 'G':'magenta', 'V':'lime',\n",
    "          'O':'teal', 'F':'black', 'Q':'yellow', 'T':'lightcoral', 'KL':'brown', 'K':'peachpuff', 'FP':'plum', \n",
    "          'MU':'peru', 'KLT':'sienna', 'KT':'maroon', 'NU':'indigo', 'U':'darkolivegreen',\n",
    "          'GM':'crimson', 'NOU':'thistle', 'NPTU':'dodgerblue', 'EGP':'firebrick', 'IQ':'mediumslateblue', \n",
    "          'PT':'lightsalmon', 'D':'khaki', 'OU':'darkseagreen', 'ET':'lawngreen', 'CO':'sandybrown',\n",
    "          'CH':'tan', 'UW':'royalblue', 'HP':'goldenrod', 'EQ':'springgreen', 'HJ':'cadetblue', 'CDZ':'pink', \n",
    "          'FG':'slategrey'}\n",
    "\n",
    "def gbff_to_dataframe(gbff_file):\n",
    "    records = SeqIO.parse(gbff_file, \"genbank\")\n",
    "\n",
    "    data = []\n",
    "    for record in records:\n",
    "        for feature in record.features:\n",
    "            if feature.type == \"CDS\":\n",
    "                row = {\n",
    "                    \"locus_tag\": feature.qualifiers.get(\"locus_tag\", [\"\"])[0],\n",
    "                    \"old_locus_tag\": feature.qualifiers.get(\"old_locus_tag\", [\"\"])[0],\n",
    "                    \"gene\": feature.qualifiers.get(\"gene\", [\"\"])[0],\n",
    "                    \"product\": feature.qualifiers.get(\"product\", [\"\"])[0],\n",
    "                    \"Start\": feature.location.start,\n",
    "                    \"End\": feature.location.end,\n",
    "                    \"Orientation\": feature.location.strand,\n",
    "                }\n",
    "                if row[\"Orientation\"] == 1:\n",
    "                    row[\"Orientation\"] == \"+\"\n",
    "                else:\n",
    "                    row[\"Orientation\"] == \"-\"\n",
    "                data.append(row)\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def Load_Prodigal_GBFF(filepath):\n",
    "    lines = open(filepath).readlines()\n",
    "    op = []\n",
    "    ctr = 1\n",
    "    for i in range(len(lines)):\n",
    "        l = lines[i]\n",
    "        l = l.replace(\"\\n\",\"\")\n",
    "        l = l.strip()\n",
    "        if l == \"//\":\n",
    "            ctr = 1\n",
    "        else:\n",
    "            if l.startswith(\"DEFINITION\"):\n",
    "                l = l.replace(\"DEFINITION \",\"\")\n",
    "                splits = l.split(\";\")\n",
    "                for s in splits:\n",
    "                    if s.startswith(\"seqhdr\"):\n",
    "                        sequence_id = s.replace(\"seqhdr=\",\"\").replace(\"\\\"\",\"\")\n",
    "                    if s.startswith(\"seqlen\"):\n",
    "                        seqlen = int(s.replace(\"seqlen=\",\"\").replace(\"\\\"\",\"\"))\n",
    "            if l.startswith(\"CDS\"):\n",
    "                l = l.replace(\"CDS\",\"\").strip()\n",
    "                if l.startswith(\"complement\"):\n",
    "                    orientation = '-'\n",
    "                else:\n",
    "                    orientation = '+'\n",
    "                l = l.replace(\"complement\",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\"<\",\"\").replace(\">\",\"\")\n",
    "                start,end = l.split(\"..\")\n",
    "                i += 1\n",
    "                record = lines[i]\n",
    "                record = record.strip()\n",
    "                if record.startswith(\"/note\"):\n",
    "                    record = record.replace(\"/note=\",\"\").replace(\"\\\"\",\"\")\n",
    "                    splits = record.split(\";\")\n",
    "                    d = {'Query':sequence_id, 'Pred':sequence_id+'_'+str(ctr), 'Qlen':seqlen, \n",
    "                         'Orientation':orientation, 'Start':start, 'End':end}\n",
    "                    for s in splits[:-1]:\n",
    "                        s = s.replace(\"\\n\",\"\")\n",
    "                        key, value = s.split('=')\n",
    "                        d[key] = value\n",
    "                    ctr += 1\n",
    "                    op.append(d)\n",
    "    df_Prodigal_Hits = pd.DataFrame(op)\n",
    "    df_Prodigal_Hits['Start'] = df_Prodigal_Hits['Start'].astype(int)\n",
    "    df_Prodigal_Hits['End'] = df_Prodigal_Hits['End'].astype(int)\n",
    "    return df_Prodigal_Hits[['Query','Pred','Orientation','Start','End','Qlen','ID','partial','conf']]\n",
    "\n",
    "def Make_Counts(df_osa, df_osb, osa_len, osb_len):\n",
    "    osa_indicator = {}\n",
    "    osb_indicator = {}\n",
    "    osa_gbl_counts = np.zeros(osa_len)\n",
    "    osb_gbl_counts = np.zeros(osb_len)\n",
    "    try: osa_samples = set(df_osa['Sample'].tolist())\n",
    "    except KeyError: osa_samples = set({})\n",
    "    try: osb_samples = set(df_osb['Sample'].tolist())\n",
    "    except KeyError: osb_samples = set({})\n",
    "        \n",
    "    for g in osa_samples:\n",
    "        try:\n",
    "            osa = np.zeros(osa_len)\n",
    "            temp_osa = df_osa[df_osa['Sample'] == g]\n",
    "            starts, ends, lengths = temp_osa['Start'].tolist(), temp_osa['End'].tolist(), temp_osa['Length'].tolist()\n",
    "            \n",
    "            for i in range(len(starts)):\n",
    "                start, end = int(min(starts[i],ends[i])), int(max(starts[i],ends[i]))\n",
    "                if abs(start-end) == lengths[i]:    \n",
    "                    osa[start:end] += 1\n",
    "                    osa_gbl_counts[start:end] += 1\n",
    "                else:\n",
    "                    osa[end:osa_len] += 1\n",
    "                    osa[0:start] += 1\n",
    "                    osa_gbl_counts[end:osa_len] += 1\n",
    "                    osa_gbl_counts[0:start] += 1\n",
    "            osa[np.isnan(osa)] = 0\n",
    "            osa_indicator[g] = osa\n",
    "        except KeyError: osa_indicator[g] = osa\n",
    "            \n",
    "    for g in osb_samples:    \n",
    "        try:\n",
    "            osb = np.zeros(osb_len)\n",
    "            temp_osb = df_osb[df_osb['Sample'] == g].reset_index()\n",
    "            starts, ends, lengths = temp_osb['Start'].tolist(), temp_osb['End'].tolist(), temp_osb['Length'].tolist()\n",
    "            \n",
    "            for i in range(len(starts)):\n",
    "                start, end = int(min(starts[i],ends[i])), int(max(starts[i],ends[i]))\n",
    "                if abs(start-end) == lengths[i]:    \n",
    "                    osb[start:end] += 1\n",
    "                    osb_gbl_counts[start:end] += 1\n",
    "                else:\n",
    "                    osb[end:osa_len] += 1\n",
    "                    osb[0:start] += 1\n",
    "                    osb_gbl_counts[end:osb_len] += 1\n",
    "                    osb_gbl_counts[0:start] += 1\n",
    "            osb[np.isnan(osb)] = 0\n",
    "            osb_indicator[g] = osb\n",
    "        except KeyError: osb_indicator[g] = osb\n",
    "    return osa_indicator, osb_indicator, osa_gbl_counts, osb_gbl_counts\n",
    "\n",
    "def Make_Circle_Plots(ax, values, color, text, ylim, lw = 5):\n",
    "    angles = np.linspace(0, 2*np.pi, len(values), endpoint=False)\n",
    "    ax.plot(angles, values, linewidth=lw, color = color)\n",
    "    ax.grid(False)\n",
    "    ax.set_ylim([0, ylim])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticks([])\n",
    "    ax.text(0,0, text, horizontalalignment='center', verticalalignment='center',size = 20)\n",
    "    ax.spines['polar'].set_visible(False)\n",
    "\n",
    "def Plot_Gene_Annotations(df, ax, width, idx, start, end, offset=2500, color = 'gray'):\n",
    "    filt_start = min(start, end)-offset\n",
    "    filt_end = filt_start+offset+width+offset\n",
    "    df_filt = df.loc[(df['Start'] >= filt_start) & (df['End'] <= filt_end)].copy()\n",
    "    df_filt['Start'] = df_filt['Start'] - min(start, end)\n",
    "    df_filt['End'] = df_filt['End'] - min(start, end)\n",
    "    df_gene = df_filt[df_filt[\"gene\"]!=\"\"].copy()\n",
    "    df_gene['annotation'] = df_gene[\"gene\"]\n",
    "    df_empty = df_filt[df_filt[\"gene\"]==\"\"].copy()\n",
    "    df_empty['annotation'] = df_empty[\"product\"]\n",
    "    df_flanking = pd.concat([df_gene, df_empty], axis = 0)\n",
    "\n",
    "    ax.plot([-1*offset, width+offset],[idx,idx], linewidth = 12, color = color, alpha = 0.6)\n",
    "    starts = df_flanking['Start'].tolist()\n",
    "    ends =  df_flanking['End'].tolist()\n",
    "    orientations = df_flanking['Orientation'].tolist()\n",
    "    annotation = df_flanking['annotation'].tolist()\n",
    "    #if \"seed_ortholog\" in \n",
    "    for i in range(0, len(starts)):\n",
    "        if orientations[i] == 1:\n",
    "            ax.arrow(starts[i],idx-1,ends[i]-starts[i]+1,0, width = 0.15, \n",
    "                     length_includes_head = True, head_length = 150)\n",
    "        else:\n",
    "            ax.arrow(ends[i],idx-1,starts[i]-ends[i]+1,0, width = 0.15, \n",
    "                    length_includes_head = True, head_length = 150)\n",
    "        loc = starts[i]\n",
    "        if \" \" not in annotation[i]: loc = starts[i]+(ends[i]-starts[i])/4\n",
    "        ax.text(loc, idx-1+0.25, annotation[i], size = 12, rotation = 20)\n",
    "    idx = idx - 2\n",
    "    return ax, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbff_path = '/Users/harihara/Downloads/GCF_000013205.1_ASM1320v1_genomic.gbff'\n",
    "df_osa_gbff = gbff_to_dataframe(gbff_path)\n",
    "\n",
    "gbff_path = '/Users/harihara/Downloads/GCF_000013225.1_ASM1322v1_genomic.gbff'\n",
    "df_osb_gbff = gbff_to_dataframe(gbff_path)\n",
    "T = pd.concat([df_osa_gbff, df_osb_gbff], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_path = '/Users/harihara/Mount/Hotsprings_Variant_Structure_Data_Analysis/Synechococcus/containment_clusters.txt'\n",
    "df_novel_filtered = pd.read_csv(grp_path, sep = \"\\t\")\n",
    "d = df_novel_filtered.groupby('GroupID')['Contig'].apply(list).to_dict()\n",
    "d_representatives = df_novel_filtered.groupby('GroupID')['RepresentativeContig'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HotsprottomLayer_FD\n",
      "HotsprSampOS1260_FD\n",
      "HotsprSampleMS65_FD\n",
      "Hotspr20Samplem2_FD\n",
      "HotsprSampleOSM1_FD\n",
      "HotsprSampleMS60_FD\n",
      "HotsprSampleOSM3_FD\n",
      "HotsprSampleMSe1_FD\n",
      "HotsprSampleOSM2_FD\n",
      "HotsprSampleMS55_FD\n",
      "HotsprSampleOS65_FD\n",
      "Hotspr20Samplet1_FD\n",
      "HotsprSampOS1265_FD\n",
      "Hotspr20SampleT9_FD\n",
      "HotsprSampleOS55_FD\n",
      "Hotspr2Sampleee2_FD\n",
      "Hotspr2Sample149_FD\n",
      "HotsprSampleOS50_FD\n",
      "Hotspr2Sample148_FD\n",
      "HotsprOSTMatCore_FD\n",
      "HotsprottomLayer_2_FD\n",
      "HotsprSampleOSM4_FD\n",
      "HotsprSampleMS13_FD\n",
      "HotsprSampleMSe4_FD\n",
      "Hotspr20SampleP4_FD\n",
      "HotsprSampleOS60_FD\n",
      "HotsprSampleMS50_FD\n",
      "HotsprSampleMSe2_FD\n",
      "Hotspr20SampleT8_FD\n",
      "Hotspr2SamplePe2_FD\n",
      "HotsprSampleMSe3_FD\n",
      "HotsprSampleR4cd_FD\n",
      "Hotspr2Sampleme2_FD\n",
      "HotsprSamplt10cd_FD\n"
     ]
    }
   ],
   "source": [
    "novel_contigs = {}\n",
    "novel_contig_path = '/Users/harihara/Mount-2/hotspring_metagenome/Synechococcus_paper_analysis/\\\n",
    "Hotsprings_Variant_Structure/'\n",
    "\n",
    "samples = listdir(novel_contig_path+'OSA/')\n",
    "for s in samples:\n",
    "    if s.startswith(\"Hot\"):\n",
    "        print(s)\n",
    "        df_osa = pd.read_csv(f\"{novel_contig_path}OSA/{s}/reference_guided_scaffolds/Ref_Guided_Scaffolds.OSA.txt\", \n",
    "                             sep = \"\\t\")\n",
    "        df_osa.loc[(df_osa['Start'] < 0), 'Start'] += osa_len\n",
    "        df_osa.loc[(df_osa['End'] < 0), 'End'] += osa_len\n",
    "        df_osa_grp = df_osa.sort_values(by = ['Contig','Start']).groupby(['Contig']).apply(Max_Clique_Interval_Graph)\n",
    "    \n",
    "        df_osb = pd.read_csv(f\"{novel_contig_path}OSB/{s}/reference_guided_scaffolds/Ref_Guided_Scaffolds.OSB.txt\", \n",
    "                             sep = \"\\t\")\n",
    "        df_osb.loc[(df_osb['Start'] < 0), 'Start'] += osb_len\n",
    "        df_osb.loc[(df_osb['End'] < 0), 'End'] += osb_len\n",
    "        df_osb_grp = df_osb.sort_values(by = ['Contig','Start']).groupby(['Contig']).apply(Max_Clique_Interval_Graph)\n",
    "        novel_contigs[s.replace(\".txt\",\"\")] = {'OSA':df_osa_grp,'OSB':df_osb_grp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prodigal_out = '/Users/harihara/Mount/Hotsprings_Variant_Structure_Data_Analysis/Synechococcus/Prodigal/\\\n",
    "Representatives_Prodigal.out'\n",
    "df_prodigal_hits = Load_Prodigal_GBFF(prodigal_out)\n",
    "df_prodigal_hits = df_prodigal_hits.set_index('Pred')\n",
    "\n",
    "eggnog_path = '/Users/harihara/Mount/Hotsprings_Variant_Structure_Data_Analysis/Synechococcus/EggNOG/\\\n",
    "Representatives.eggnog.out.emapper.annotations'\n",
    "df_eggnog = pd.read_csv(eggnog_path, sep = \"\\t\")\n",
    "df_eggnog['RepresentativeContig'] = df_eggnog['#query'].apply(get_Query_ID)\n",
    "df_eggnog = df_eggnog.rename(columns = {'#query':'Query'})\n",
    "df_eggnog = df_eggnog.set_index('Query')\n",
    "df_eggnog['COG_category'] = df_eggnog['COG_category'].fillna(\"-\")\n",
    "df_eggnog['Description'] = df_eggnog['Description'].fillna(\"Unspecified\")\n",
    "df_eggnog[['seed_0','seed_1']] = df_eggnog['seed_ortholog'].str.rsplit(\".\", n=1, expand = True)\n",
    "df_prodigal_hits = df_prodigal_hits.join(df_eggnog)\n",
    "df_prodigal_hits = df_prodigal_hits.reset_index()\n",
    "df_prodigal_hits = df_prodigal_hits.merge(df_novel_filtered[['RepresentativeContig','GroupID']].drop_duplicates(),\n",
    "                                          left_on = 'Query', right_on='RepresentativeContig', how = 'left')\n",
    "df_prodigal_hits = pd.merge(df_prodigal_hits, T[['old_locus_tag','product','gene']], \n",
    "                            left_on = 'seed_1', right_on = 'old_locus_tag', how = \"outer\")\n",
    "df_prodigal_hits['gene'] = df_prodigal_hits['gene'].fillna(df_prodigal_hits['Preferred_name'])\n",
    "df_prodigal_hits['gene'] = df_prodigal_hits['gene'].fillna(\"\")\n",
    "df_prodigal_hits['product'] = df_prodigal_hits['product'].fillna(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hotspr20SampleP4_FD_OSA_k141_10411'] ['OSA'] 729 17.55829903978052\n",
      "['Hotspr20SampleP4_FD_OSA_k141_10411'] ['OSB'] 729 99.86282578875172\n",
      "['HotsprSampleMS13_FD_OSB_k141_51304'] ['OSB'] 729 99.86282578875172\n"
     ]
    }
   ],
   "source": [
    "synechococcus_blast = '/Users/harihara/Mount/Hotsprings_Variant_Structure_Data_Analysis/Synechococcus/\\\n",
    "Representatives.Synechococcus.blast'\n",
    "\n",
    "df_blast = pd.read_csv(synechococcus_blast, sep = \"\\t\",\n",
    "                       names=['qseqid', 'sseqid', 'pident', 'length', 'mismatch', 'gapopen', 'qlen', \n",
    "                              'qstart', 'qend', 'slen', 'sstart', 'send', 'evalue', 'bitscore'])\n",
    "df_blast = df_blast.merge(df_novel_filtered[['RepresentativeContig','GroupID']].drop_duplicates(),\n",
    "                          left_on = 'qseqid', right_on='RepresentativeContig', how = 'left')\n",
    "df_blast['sseqid'] = df_blast['sseqid'].replace(\"gi|86604733|ref|NC_007775.1|\",\"OSA\")\n",
    "df_blast['sseqid'] = df_blast['sseqid'].replace(\"gi|86607503|ref|NC_007776.1|\",\"OSB\")\n",
    "\n",
    "grouped = df_blast.rename(columns = {'sseqid':'Genome'}).groupby(['qseqid','Genome'])\n",
    "df_blast_grouped = pd.DataFrame()\n",
    "df_blast_grouped['QCov'] = grouped.apply(Compute_Query_Coverage)\n",
    "df_blast_grouped = df_blast_grouped.reset_index().pivot(index = ['qseqid'], columns=['Genome'], values=['QCov'])\n",
    "df_blast_grouped = df_blast_grouped.fillna(0)\n",
    "df_blast_grouped.columns = df_blast_grouped.columns.droplevel()\n",
    "df_blast_grouped = df_blast_grouped[(df_blast_grouped['OSA'] < 75) & (df_blast_grouped['OSB'] < 75)]\n",
    "representative_short_listed = df_blast_grouped.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '/Users/harihara/Research-Activities/Plots/Hot_Spring_Plots/Synechococcus-Paper-New-Plots/'\n",
    "if not isdir(out_dir):\n",
    "    mkdir(out_dir)\n",
    "if not isdir(out_dir+'Novel_Groups_11102024/'):\n",
    "    mkdir(out_dir+'Novel_Groups_11102024/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group_555\n",
      "Group_665\n",
      "Group_835\n",
      "Group_1035\n",
      "Group_1160\n",
      "Not in OSB\n",
      "Group_1206\n",
      "Group_1280\n",
      "Group_1287\n",
      "Group_1341\n",
      "Group_1365\n",
      "Group_1390\n",
      "Group_1501\n",
      "Group_1565\n",
      "Group_1580\n",
      "Group_1635\n",
      "Not in OSB\n",
      "Group_1673\n",
      "Group_1674\n",
      "Not in OSA\n",
      "Group_1708\n",
      "Not in OSA\n",
      "Group_1740\n",
      "Group_1775\n",
      "Group_1829\n",
      "Group_1842\n",
      "Not in OSB\n",
      "Group_1855\n",
      "Not in OSA\n",
      "Group_1862\n",
      "Not in OSA\n",
      "Group_1903\n",
      "Group_1937\n",
      "Not in OSA\n",
      "Group_1947\n",
      "Group_1949\n",
      "Not in OSA\n",
      "Group_2033\n",
      "Not in OSB\n",
      "Group_2061\n",
      "Not in OSA\n",
      "Group_2074\n",
      "Group_2079\n",
      "Not in OSA\n",
      "Group_2080\n",
      "Not in OSB\n",
      "Group_2081\n",
      "Not in OSA\n",
      "Group_2086\n",
      "Not in OSB\n",
      "Group_2095\n",
      "Group_2183\n",
      "Not in OSA\n",
      "Group_2200\n",
      "Group_2235\n",
      "Not in OSB\n",
      "Group_2266\n",
      "Not in OSB\n",
      "Group_2270\n",
      "Group_2276\n",
      "Not in OSA\n",
      "Group_2294\n",
      "Not in OSA\n",
      "Group_2299\n",
      "Group_2323\n",
      "Not in OSA\n",
      "Group_2343\n",
      "Group_2353\n",
      "Group_2359\n",
      "Group_2388\n",
      "Group_2398\n",
      "Not in OSA\n",
      "Group_2401\n",
      "Not in OSB\n",
      "Group_2422\n",
      "Not in OSB\n",
      "Group_2433\n",
      "Not in OSA\n",
      "Group_2442\n",
      "Group_2477\n",
      "Group_2493\n",
      "Group_2523\n",
      "Not in OSB\n",
      "Group_2548\n",
      "Not in OSB\n",
      "Group_2551\n",
      "Group_2565\n",
      "Group_2594\n",
      "Not in OSA\n",
      "Group_2596\n",
      "Not in OSA\n",
      "Group_2600\n",
      "Not in OSB\n",
      "Group_2619\n",
      "Group_2620\n",
      "Group_2635\n",
      "Group_2700\n",
      "Not in OSB\n",
      "Group_2702\n",
      "Not in OSA\n",
      "Group_2708\n",
      "Group_2724\n",
      "Not in OSB\n",
      "Group_2725\n",
      "Group_2733\n",
      "Group_2745\n",
      "Not in OSB\n",
      "Group_2750\n",
      "Not in OSA\n",
      "Group_2765\n",
      "Group_2782\n",
      "Group_2790\n",
      "Group_2795\n",
      "Not in OSA\n",
      "Group_2796\n",
      "Group_2797\n",
      "Not in OSB\n",
      "Group_2799\n",
      "Group_2806\n",
      "Not in OSB\n",
      "Group_2820\n",
      "Not in OSA\n",
      "Group_2821\n",
      "Not in OSB\n",
      "Group_2822\n",
      "Not in OSA\n",
      "Group_2829\n",
      "Not in OSB\n",
      "Group_2841\n",
      "Group_2842\n",
      "Group_2845\n",
      "Not in OSA\n",
      "Group_2852\n",
      "Not in OSB\n",
      "Group_2876\n",
      "Not in OSA\n",
      "Group_2881\n",
      "Group_2888\n",
      "Not in OSA\n",
      "Group_2905\n",
      "Not in OSB\n",
      "Group_2907\n",
      "Group_2914\n",
      "Group_2925\n",
      "Not in OSB\n",
      "Group_2926\n",
      "Group_2950\n",
      "Group_2966\n",
      "Group_2984\n",
      "Not in OSB\n",
      "Group_2994\n",
      "Not in OSB\n",
      "Group_2998\n",
      "Not in OSA\n",
      "Group_3008\n",
      "Group_3028\n",
      "Group_3035\n",
      "Group_3036\n",
      "Group_3038\n",
      "Group_3043\n",
      "Group_3055\n",
      "Group_3056\n",
      "Group_3086\n",
      "Group_3087\n",
      "Group_3113\n",
      "Not in OSA\n",
      "Group_3114\n",
      "Not in OSA\n",
      "Group_3132\n",
      "Group_3157\n",
      "Group_3180\n",
      "Group_3197\n",
      "Not in OSB\n",
      "Group_3206\n",
      "Not in OSB\n",
      "Group_3213\n",
      "Not in OSB\n",
      "Group_3231\n",
      "Group_3244\n",
      "Not in OSB\n",
      "Group_3263\n",
      "Group_3266\n",
      "Group_3267\n",
      "Group_3272\n",
      "Not in OSB\n",
      "Group_3288\n",
      "Not in OSB\n",
      "Group_3291\n",
      "Group_3298\n",
      "Group_3304\n",
      "Group_3315\n",
      "Group_3325\n",
      "Not in OSA\n",
      "Group_3341\n",
      "Group_3351\n",
      "Not in OSA\n",
      "Group_3357\n",
      "Not in OSA\n",
      "Group_3367\n",
      "Not in OSA\n",
      "Group_3381\n",
      "Not in OSA\n",
      "Group_3390\n",
      "Not in OSA\n",
      "Group_3391\n",
      "Not in OSA\n",
      "Group_3392\n",
      "Not in OSA\n",
      "Group_3393\n",
      "Not in OSA\n",
      "Group_3397\n",
      "Group_3403\n",
      "Not in OSA\n",
      "Group_3404\n",
      "Not in OSA\n",
      "Group_3416\n",
      "Not in OSA\n",
      "Group_3417\n",
      "Not in OSA\n",
      "Group_3426\n",
      "Not in OSA\n",
      "Group_3427\n",
      "Group_3433\n",
      "Not in OSA\n",
      "Group_3442\n",
      "Not in OSA\n",
      "Group_3450\n",
      "Not in OSA\n",
      "Group_3464\n",
      "Group_3465\n",
      "Group_3472\n",
      "Group_3475\n",
      "Not in OSA\n",
      "Group_3476\n",
      "Not in OSA\n",
      "Group_3483\n",
      "Not in OSA\n",
      "Group_3494\n",
      "Not in OSA\n",
      "Group_3496\n",
      "Not in OSA\n",
      "Group_3498\n",
      "Not in OSA\n",
      "Group_3505\n",
      "Not in OSA\n",
      "Group_3513\n",
      "Not in OSA\n",
      "Group_3516\n",
      "Not in OSA\n",
      "Group_3519\n",
      "Not in OSA\n",
      "Group_3521\n",
      "Not in OSA\n",
      "Group_3523\n",
      "Not in OSA\n",
      "Group_3539\n",
      "Group_3545\n",
      "Not in OSB\n",
      "Group_3546\n",
      "Group_3587\n",
      "Not in OSA\n",
      "Group_3588\n",
      "Not in OSA\n",
      "Group_3605\n",
      "Not in OSA\n",
      "Group_3612\n",
      "Not in OSA\n",
      "Group_3617\n",
      "Not in OSA\n",
      "Group_3618\n",
      "Not in OSA\n",
      "Group_3623\n",
      "Not in OSB\n",
      "Group_3624\n",
      "Group_3628\n",
      "Group_3629\n",
      "Not in OSB\n",
      "Group_3631\n",
      "Not in OSB\n",
      "Group_3633\n",
      "Group_3634\n",
      "Not in OSB\n",
      "Group_3641\n",
      "Not in OSA\n",
      "Group_3645\n",
      "Not in OSA\n",
      "Group_3646\n",
      "Not in OSA\n",
      "Group_3651\n",
      "Not in OSA\n",
      "Group_3653\n",
      "Not in OSA\n",
      "Group_3656\n",
      "Not in OSA\n",
      "Group_3658\n",
      "Not in OSA\n",
      "Group_3678\n",
      "Group_3690\n",
      "Not in OSA\n",
      "Group_3695\n",
      "Group_3698\n",
      "Not in OSA\n",
      "Group_3700\n",
      "Not in OSA\n",
      "Group_3701\n",
      "Not in OSA\n",
      "Group_3708\n",
      "Group_3713\n",
      "Not in OSA\n",
      "Group_3714\n",
      "Group_3725\n",
      "Not in OSA\n",
      "Group_3727\n",
      "Group_3734\n",
      "Not in OSA\n",
      "Group_3735\n",
      "Not in OSA\n",
      "Group_3742\n",
      "Group_3745\n",
      "Not in OSA\n"
     ]
    }
   ],
   "source": [
    "plt.rcParams.update(rcParams)\n",
    "OSA_All_Counts, OSB_All_Counts = {}, {}\n",
    "filtered_grp = df_novel_filtered[df_novel_filtered['RepresentativeContig'].isin(representative_short_listed)]\n",
    "\n",
    "for g in list(filtered_grp['GroupID'].unique()):    \n",
    "    print(g)\n",
    "    contigs = d[g]+[d_representatives[g][0]]\n",
    "    osa_contig_count, osb_contig_count = set({}), set({})\n",
    "    osa_contigs, osb_contigs = [], []\n",
    "    df_osa, df_osb = pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    for c in contigs:\n",
    "        splits = c.split('_')\n",
    "        contig = splits[-2]+'_'+splits[-1]\n",
    "        genome = splits[-3].upper()\n",
    "        sample = \"_\".join(splits[:-3])\n",
    "        \n",
    "        if genome == \"OSA\": osa_contig_count.add(sample)\n",
    "        if genome == \"OSB\": osb_contig_count.add(sample)\n",
    "        \n",
    "        try:\n",
    "            row = novel_contigs[sample][genome].loc[contig]\n",
    "            row['Group'] = g\n",
    "            row['Sample'] = sample\n",
    "            if genome == 'OSA': df_osa = df_osa.append(row)\n",
    "            elif genome == 'OSB': df_osb = df_osb.append(row)\n",
    "        except:\n",
    "            print(\"Missing...\",sample,genome,contig)\n",
    "            pass\n",
    "    \n",
    "    if len(df_osa) > 0: df_osa = df_osa.reset_index()\n",
    "    if len(df_osb) > 0: df_osb = df_osb.reset_index()\n",
    "    \n",
    "    osa_counts, osb_counts, osa_gbl_counts, osb_gbl_counts = Make_Counts(df_osa, df_osb, osa_len, osb_len)\n",
    "    osa_gbl_counts += 30\n",
    "    osb_gbl_counts += 30\n",
    "    for o in osa_counts:\n",
    "        try: OSA_All_Counts[o] += osa_counts[o]\n",
    "        except KeyError: OSA_All_Counts[o] = osa_counts[o]\n",
    "    \n",
    "    for o in osb_counts:\n",
    "        try: OSB_All_Counts[o] += osb_counts[o]\n",
    "        except KeyError: OSB_All_Counts[o] = osb_counts[o]\n",
    "            \n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    gs = GridSpec(nrows=2, ncols=2, height_ratios=[2.25, 3])\n",
    "    ax0 = fig.add_subplot(gs[0, 0], projection = 'polar')\n",
    "    ax1 = fig.add_subplot(gs[0, 1], projection = 'polar')\n",
    "    ax2 = fig.add_subplot(gs[1, :])\n",
    "    \n",
    "#     osa_alignments_starts = df_blast.loc[(df_blast['GroupID'] == g) & (df_blast['sseqid'] == 'OSA') ,\n",
    "#                                          'qstart'].tolist()\n",
    "#     osa_alignments_ends = df_blast.loc[(df_blast['GroupID'] == g) & (df_blast['sseqid'] == 'OSA') ,\n",
    "#                                          'qend'].tolist()\n",
    "    \n",
    "#     osb_alignments_starts = df_blast.loc[(df_blast['GroupID'] == g) & (df_blast['sseqid'] == 'OSB') ,\n",
    "#                                          'qstart'].tolist()\n",
    "#     osb_alignments_ends = df_blast.loc[(df_blast['GroupID'] == g) & (df_blast['sseqid'] == 'OSB') ,\n",
    "#                                          'qend'].tolist()\n",
    "    try: width = df_prodigal_hits.loc[df_prodigal_hits['GroupID'] == g, 'Qlen'].tolist()[0]\n",
    "    except IndexError: continue\n",
    "    idx = 6\n",
    "    try:\n",
    "        df_osa = df_osa.sort_values(by = 'Length', ascending = False)\n",
    "        osa_start, osa_end = df_osa.iloc[0]['Start'], df_osa.iloc[0]['End']\n",
    "        ax2, idx = Plot_Gene_Annotations(df_osa_gbff, ax2, width, idx, osa_start, osa_end, color = 'green')\n",
    "    except KeyError:\n",
    "        print(\"Not in OSA\")\n",
    "        \n",
    "    try:\n",
    "        df_osb = df_osb.sort_values(by = 'Length', ascending = False)\n",
    "        osb_start, osb_end = df_osb.iloc[0]['Start'], df_osb.iloc[0]['End']\n",
    "        ax2, idx = Plot_Gene_Annotations(df_osb_gbff, ax2, width, idx, osb_start, osb_end, color = 'orange')\n",
    "    except KeyError:\n",
    "        print(\"Not in OSB\")\n",
    "    \n",
    "    ax2, idx = Plot_Gene_Annotations(df_prodigal_hits.loc[df_prodigal_hits['GroupID'] == g], \n",
    "                                     ax2, width, idx, 0, width, offset = 0)   \n",
    "    ax2.set_yticks([])\n",
    "    \n",
    "    ylim = max(np.max(osa_gbl_counts), np.max(osb_gbl_counts))\n",
    "    Make_Circle_Plots(ax0,osa_gbl_counts,'green', f\"Syn. OSA\\n#Samples :{len(osa_contig_count)}\", ylim)\n",
    "    Make_Circle_Plots(ax1,osb_gbl_counts,'orange', f\"Syn. OSB\\n#Samples :{len(osb_contig_count)}\", ylim)\n",
    "    fig.suptitle(\"\\n\"+g.replace(\"_\",\" \"))\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_dir+\"Novel_Groups_11102024/\"+g+\".pdf\")\n",
    "    plt.close(\"all\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group_3465 not found\n",
      "Group_3678 not found\n",
      "Group_3714 not found\n"
     ]
    }
   ],
   "source": [
    "import resource \n",
    "from PyPDF2 import PdfMerger\n",
    "\n",
    "soft, hard = resource.getrlimit(resource.RLIMIT_NOFILE)\n",
    "resource.setrlimit(resource.RLIMIT_NOFILE, (hard, hard))\n",
    "\n",
    "pdfs = list(filtered_grp['GroupID'].unique())\n",
    "merger = PdfMerger()\n",
    "for pdf in pdfs:\n",
    "    if pdf.startswith('Group'):\n",
    "        try: merger.append(out_dir+'Novel_Groups_11102024/'+pdf+'.pdf')\n",
    "        except FileNotFoundError: print(pdf, 'not found')\n",
    "        \n",
    "merger.write(out_dir+'Gene_Plots.pdf')\n",
    "merger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
